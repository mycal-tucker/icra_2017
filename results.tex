\section{Evaluation}
\label{sec:evaluation}
%\subsection{Experimental Setup}
The performance of the DCG-UPUP-Away model was demonstrated in two experiments.
First, a simulated TurtleBot within randomly generated simulated environments was given a series of user-generated natural language commands.
Second, an actual TurtleBot was given specific commands in a laboratory environment in order to demonstrate novel behaviors enabled by the DCG-UPUP-Away model.
Both experiments assumed a perfect object recognizer that translates the raw sensor data into symbols into the world model, as well as an initial set of hand-labeled training examples for training the log-linear model to ground cubes, spheres, and cylinders.
In all trials, training the model with $55$ positive examples took less than $1$ minute on a Lenovo Thinkpad X1 Carbon, and grounding a command took under $40$ seconds.
%\subsection{Experimental Setup}

\subsection{Corpus}
The simulated testing environments were randomly generated in Gazebo.
Ten worlds were created, and each was populated with a random collection of objects in randomized locations.
In this work, we considered $8$ possible object types (including cubes, spheres, and cylinders) in $3$ possible colors (red, blue, green), for a total of $24$ objects.
Each object had a $15\%$ chance of being added to a given map.
Using such a procedure to generate environments coupled with the limited field of view of the TurtleBot caused $87\%$ of the objects to be placed outside the initial field of view of the robot, which demonstrates the need for the ability to ground commands to hypothesized objects.

After generating the 10 worlds, the screen shots of a world with a highlighted single object were uploaded to Amazon Mechanical Turk. For each image, the users were instructed to write a command ``for approaching the highlighted object.''
These image-command pairs were saved for evaluating whether a robot, when placed in the corresponding simulated world and given the natural language command, successfully approached the correct object.
An example screen shot, with an annotation supplied by a user, is shown in Fig.~\ref{fig:amt}.
\begin{figure}[b!]
	\centering
    \includegraphics[width=6cm]{amt}
	\caption{A simulated world with a highlighted object presented on Amazon Mechanical Turk, labeled by a user as ``Move to the red fire hydrant.''}
	\label{fig:amt}
\end{figure}
\subsection{Testing/Training Setup}
Ten image-command pairs were randomly selected without any replacement from the pool of all pairs.
Note that one iteration corresponds to a specific pair, and $10$ ordered pairs is called a trial. 
Accordingly, $30$ trials were generated, each consisting of $10$ iterations, for a total of $300$ evaluations.
When executing a trial, the TurtleBot was first trained on the initial, hand-curated training set.
The robot was then given the natural language command from the first iteration, and then retrained using the initial data supplemented by unsupervised training examples generated by the first iteration.
The retrained TurtleBot was given the command from the next iteration, and appropriately retrained after each execution until all 10 iterations have been executed.

\subsection{Perception Pipeline}
The perception pipeline involved object detection using the \emph{Edge Boxes} toolbox of Zitnick, Lawrence and Doll{'a}r \cite{zitnick2014edge} in which edges within an image are identified and grouped into candidate objects. Dense SIFT features \cite{bosch2007image} were extracted from the set of training images (with the VLFeat \cite{vedaldi2010vlfeat} toolbox) and used to construct a dictionary (of size 600). Spatial histograms over the dictionary of visual words was used to form a feature vector for the images, and the resulting vectors were then classified using a set of one-vs-all support vector machines ($\chi^2$ kernel) \cite{fan2008liblinear}. Objects were labeled as "unknown" if none of their computed signed-margins exceeded a \emph{margin threshold parameter}, which we chose to be 0.2.\footnote{It should be noted that, while perception is not the focus of this work, there is potential for improvement in both the bounding box proposal (e.g. using the depth images) and the classification.}

Images were obtained using the RGB camera from a Microsoft Kinect. We picked a subset of common objects from the standard YCB Object Dataset \cite{calli2015ycb} for evaluation. The system was initialized with knowledge of three different objects (a Cheeze-It box, a Spam tin, and a Coffee can) and with twenty training images per class. When a new object was learned, twenty images were collected (offline) for that class and the classifiers were retrained. Finally, whenever objects were recognized, the new detections were added to the training set, and the classifiers were again retrained.
\section{Results}
\begin{figure*}[htb!]
%\begin{figure}[htb!]
\centering
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[trim=1cm 0cm 0.5cm 0cm, clip=true, width=\textwidth]{grounding_accuracy.eps}
\caption{The overall grounding accuracy rate over various iterations.}
\label{fig:g_acc}
\end{subfigure}
~
\begin{subfigure}[b]{0.33\textwidth}
\centering
\includegraphics[trim=1cm 0cm 0.5cm 0.5cm, clip=true, width=\textwidth]{split_cases.eps}
\caption{The mean percentage of known, learned, and unknown symbols during the simulations.}
\label{fig:g_acc_split}
\end{subfigure}
~
%\begin{figure}[htb!]
\begin{subfigure}[b]{0.31\textwidth}
\centering
\includegraphics[trim=1cm 0.5cm 2cm 1cm, clip=true, width=\textwidth]{fig6}
\caption{The number of new symbols acquired during the simulations.}
\label{fig:symbols}
\end{subfigure}
%\caption{The performance results of the simulation study.}
%\end{figure}
%\begin{figure}[htb!]
\caption{The DCG-UPUP-Away model was used to ground $30$ trials, each of which included $10$ iterations (i.e., a specific pair of command and world model).}
\end{figure*}


This section presents the performance results of the DCG-UPUP-Away model based on the grounding accuracy (how likely the model correctly grounds a phrase) and the number of known symbols.
Under the grounding accuracy results, we also examine when phrases are grounded to known, unknown, or learned objects.
\subsection{Grounding Accuracy}
%The primary metric used in evaluating the success of DCG-UPUP-Away is the grounding accuracy: how likely is the TurtleBot to correctly execute the natural language command.
As discussed previously, the TurtleBot is retrained between the iterations, thus the grounding accuracy may change as a function of iteration number. In fact, the mean grounding accuracy remains between $70\%$ and $90\%$ across all iterations, as shown in Figure~\ref{fig:g_acc}. Although the overall grounding accuracy remains relatively constant, the underlying behavior within the DCG-UPUP-Away model changes over the course of a trial. For example, Fig.~\ref{fig:g_acc_split} illustrates 3 curves showing what fraction of correctly grounded phrases refer to known, unknown, or learned objects as a function of iteration number. In the first iteration, nearly $70\%$ of correctly grounded commands refer to known objects, but by the 10$^\text{th}$ iteration that number has fallen to nearly $10\%$, replaced almost entirely by correctly grounding to learned objects.

We also tested the performance of the DCG model with the commands containing known and unknown phrases. The DCG model was first trained with a set of commands only including phrases  ``cube", ``sphere", and ``cylinder". Then, the trained model was given $100$ commands, and $35$ of them included known phrases ``cube", ``sphere", ``cylinder" while $65$ of them contained unknown phrases ``cone", ``fire hydrant", ``cordless drill", ``mailbox", and ``door handle". The results, illustrated in Table~\ref{tab:resultDCG}, indicate that $30$ commands with known phrases were grounded correctly, $5$ commands with known phrases were grounded inaccurately, $17$ commands with unknown phrases were grounded inaccurately by associating them to wrong objects, and $48$ commands containing unknown phrases returned no grounding. This was mainly due to the solutions that could not exceed the selected confidence threshold (i.e., $0.75$ in this study). Overall, only $30$ commands out of $100$ were grounded correctly, and the results demonstrated that the DCG model performs poorly in the case of commands with unknown phrases. 

%\begin{figure}[h]
%\centering
%\includegraphics[width=8.5cm]{learning}
%\caption{this is split up (must replot well)}
%\label{fig:g_acc_split}
%\end{figure}

%\begin{figure}[b!]
%\centering
%\includegraphics[width=6.75cm]{benchmark}
%\caption{The simulation results for the DCG model initially trained with phrases ``cube", ``sphere", and ``cylinder". The trained model was used to ground $100$ commands including the known phrases as well as some unknown phrases}
%\label{fig:results_dcg}
%\end{figure}

\begin{table}[htb!]
\centering
\resizebox{0.85\columnwidth}{!}{  
\begin{tabular}{| l | c | c | c |}
\hline
 & Accurate & Inaccurate & Uninformed \\
\hline
DCG & $0.30$ & $0.22$ & $0.48$ \\
\hline
\end{tabular}
}
\caption{The summary of results showing the fraction of accurate, inaccurate, and uninformed groundings of $100$ commands.}\label{tab:resultDCG}
\end{table}

\subsection{Learned Symbols}
In order to better examine the learning behavior exhibited by the DCG-UPUP-Away model, the other performance metric considered is the number of correctly known symbols. Note that the symbols may be incorrectly learned by associating a phrase with the wrong sort of object due to the nature of unsupervised learning.
Initially, the TurtleBot is trained with cubes, spheres, and cylinders, but the generated environments may contain up to 5 additional object types (i.e., fire hydrants, drills, mailboxes, door handles, and traffic cones).
Whenever an unknown phrase is grounded to such an unknown object, the TurtleBot learns the new symbol.
Thus, one may calculate the expected number of known symbols as a function of the iteration number using combinatorics to count how many unknown objects are present.
The recorded number of correctly learned symbols are plotted in Fig.~\ref{fig:symbols} in blue, as well as the expected number in red.
%\begin{figure}[h]
%\centering
%\includegraphics[width=8.5cm]{symbols_corr}
%\caption{this is how we show that we learn symbols. I will update this label (and caption) of this figure. also must use std error bars instead of variance}
%\label{fig:symbols}
%\end{figure}
%(Asking for advice: the x axis ranges from 1 to 11 because I can retrain after the 10th iteration and increase the mean number of symbols learned. It's a small increase, though, so I wouldn't be too upset cutting off the 11th iteration, though.)

As expected, the blue curve starts at $3$ (for the cube, sphere, and cylinder), and stochastically monotonically increases.
In $10\%$ of the trials, all $8$ symbols were correctly learned. In other trials the DCG-UPUP-Away model incorrectly grounded unknown phrases (and therefore learned an incorrect symbol) or the 10 iterations collectively never referred to the five initially unknown objects, preventing the DCG-UPUP-Away model from ever learning the new symbol.
Furthermore, learning symbols correctly improves the grounding accuracy: for each additional correctly learned symbol, the TurtleBot is over $4\%$ more likely to correctly ground a command. %(TODO generate p values via ANOVA.)

\subsection{Physical Demonstration}
\begin{figure*}[t!]
\begin{subfigure}[b]{0.11\textwidth}
\centering
\includegraphics[width=\textwidth]{frame0037.jpg}
\caption{}
%\caption{t=0 sec.}
\label{fig:exp1}
\end{subfigure}
~
\begin{subfigure}[b]{0.11\textwidth}
\centering
\includegraphics[width=\textwidth]{frame0044.jpg}
\caption{}
%\caption{t=25 sec.}
\label{fig:exp2}
\end{subfigure}
~
\begin{subfigure}[b]{0.11\textwidth}
\centering
\includegraphics[width=\textwidth]{frame0045.jpg}
\caption{}
%\caption{t=50 sec.}
\label{fig:exp3}
\end{subfigure}
~
\begin{subfigure}[b]{0.11\textwidth}
\centering
\includegraphics[width=\textwidth]{frame0047.jpg}
\caption{}
%\caption{t=70 sec.}
\label{fig:exp4}
\end{subfigure}
~
\begin{subfigure}[b]{0.11\textwidth}
\centering
\includegraphics[width=\textwidth]{frame0048.jpg}
\caption{}
%\caption{t=95 sec.}
\label{fig:exp5}
\end{subfigure}
~
\begin{subfigure}[b]{0.11\textwidth}
\centering
\includegraphics[width=\textwidth]{frame0049.jpg}
\caption{}
%\caption{t=120 sec. }
\label{fig:exp6}
\end{subfigure}
~
\begin{subfigure}[b]{0.11\textwidth}
\centering
\includegraphics[width=\textwidth]{frame0050.jpg}
\caption{}
%\caption{t=120 sec. }
\label{fig:exp7}
\end{subfigure}
~
\begin{subfigure}[b]{0.11\textwidth}
\centering
\includegraphics[width=\textwidth]{frame0056.jpg}
\caption{}
%\caption{t=120 sec. }
\label{fig:exp8}
\end{subfigure}
\caption{An illustration of grounding to an unknown hypothetical object. The TurtleBot initially knew all objects in the world other than fruits. The robot was given a command as ``move to the fruits". (a) First, it did not see an unknown object in its perceived world so it created a hypothetical unknown object, (b,c,d,e,f,) it explored the world by rotating at its current location until it perceives an unknown object, (g) it perceived an unknown object and grounded to it, (h) it drove to the fruits.}
\vskip-2ex
\label{fig:crate}
\end{figure*}
In addition to the simulation studies, the DCG-UPUP-Away model was tested on an actual TurtleBot in a laboratory setting.
The TurtleBot was placed facing %a cylinder (known) and 
a box (unknown).
In addition, a jar (known), a can (known), and fruits (unknown) were located behind the TurtleBot.
%All objects were labeled with the April tags \cite{olson2011}, which were used to generate the world model $\Upsilon$ from a kinect camera mounted on the TurtleBot.

%I'd like to add a footnote saying that I have videos of these demos
Three natural language commands were used to demonstrate various capabilities of the DCG-UPUP-Away model.
%First, the TurtleBot was given the command ``move towards the cube.''
%The TurtleBot successfully drove to the cube, demonstrating a correct grounding to a known, perceived object.\\
First, the TurtleBot was given the command ``move to the box.''
The TurtleBot drove to the box, demonstrating that it perceived the box as unknown, recognized the phrase ``box'' as unknown, and grounded the unknown phrase to the unknown object.
Thus, a command was correctly grounded to an unknown perceived object as illustrated in Fig.~\ref{fig:cone}.
Second, the TurtleBot was given the command ``move to the jar''.
The TurtleBot rotated in place until the jar came in perception, and then approached the jar.
In other words, the command was first grounded to a known hypothesized object, and then it was grounded to a known perceived object once the jar was seen. 
Finally, the TurtleBot was given the command ``move to the fuits''.
Once again, the TurtleBot explored its surrounding by rotating at its current location and drove to the fruits once it perceived it (as illustrated in Fig.~\ref{fig:crate}). %rotated in place, this time until it saw the crate, whereupon it drove to the crate.

The experimental results demonstrate two important behaviors: 1) the TurtleBot must have learned what a box was, otherwise the unknown phrase ``fruits'' would have been grounded to the box, and 2) the TurtleBot grounded the command to an unknown hypothesized object until the box was perceived. %The interested reader is referred to the following link, \url{https://www.youtube.com/playlist?list=PL8sYMUToK9s6dAu3qMHHOef8FyhOnDK4E}, for the videos corresponding to these experiments.
%The execution of this last command, including images of the physical behavior of the TurtleBot as well as the model used for grounding, is shown in Figure~\ref{fig:hardware_demo}.

%\begin{figure}[ht]
%\centering
%\includegraphics[width=8.5cm]{hardware_sketch}
%\caption{this is a sketch of the 6 subfigures that demonstrate hypothesized groundings on hardware and in the model, and shows how it learns. I'd like this to go across the top of the page.}
%\label{fig:hardware_demo}
%\end{figure}
%\begin{figure*}
%\begin{subfigure}[b]{0.31\textwidth}
%\centering
%\includegraphics[width=\textwidth]{hardware1.png}
%\caption{The command ``move towards the box" is given, and the TurtleBot approaches the box by grounding a known phrase to a known object.}
%\label{fig:g_acc}
%\end{subfigure}
%~
%\begin{subfigure}[b]{0.295\textwidth}
%\centering
%\includegraphics[width=\textwidth]{hardware2.png}
%\caption{The command ``move towards the cone" is given, and the TurtleBot drives to the cone because unknown phrase ``cone" is grounded to the unknown object cone.}
%\label{fig:symbols}
%\end{subfigure}
%~
%\begin{subfigure}[b]{0.355\textwidth}
%\centering
%\includegraphics[width=\textwidth]{hardware3.png}
%\caption{The command ``move towards the cone" is given and the TurtleBot follows path 1. The command ``move towards the ball" is given, and the TurtleBot follows path 2.}
%\label{fig:g_acc_split}
%\end{subfigure}
%\caption{Some illustrations of iterative learning via the proposed model. (a) The TurtleBot initially knows a box, a helmet, and a soap box, then (b) it learns what a cone is, and then (c) it learns what a ball is.}
%\end{figure*}


\begin{figure}[htb!]
\begin{subfigure}[b]{0.305\columnwidth}
\centering
\includegraphics[width=\textwidth]{frame0058.jpg}
\caption{t= 0 sec.}
\label{fig:exp_new_1}
\end{subfigure}
~
\begin{subfigure}[b]{0.31\columnwidth}
\centering
\includegraphics[width=\textwidth]{frame0062.jpg}
\caption{t= 45 sec.}
\label{fig:exp_new_2}
\end{subfigure}
~
\begin{subfigure}[b]{0.315\columnwidth}
\centering
\includegraphics[width=\textwidth]{frame0066.jpg}
\caption{t= 50 sec.}
\label{fig:exp_new_3}
\end{subfigure}
\caption{An illustration of learning new symbol. The TurtleBot initially did not know what a box is, a command was given as ``move to the box". (a) Due to the presence of an unknown object in its perceived world, it grounded the unknown phrase ``the box" to the unknown object, (b,c) it drove to the box.}
\label{fig:cone}
\end{figure}




\subsection{Limitations}
The previous sections demonstrated that the proposed model DCG-UPUP-Away results in the successful execution of various natural language commands. This section discusses the main limitations of the model.
%of the DCG-UPUP-Away model encountrduring  are equally important in characterizing the limits of its performance.\\
In particular, the most obvious limitation of the DCG-UPUP-Away model is the assumption of referring an unknown phrase to the first perceived unknown object. %that, in the absence of additional information, unknown phrases refer to the first perceived unknown object.
%One strategy to relax this assumption has been explored in Section~\ref{sec:color} by associating language adjectives with object properties. However, a more sophisticated strategy is required for generalizable solutions.  %have been explored in Section (TODO color section), but so far only a relatively simple method has been used.\\
Moreover, the DCG-UPUP-Away model assumes a one-to-one correspondence between unknown phrases and unknown objects; thus it cannot, for example, learn synonyms by grounding unknown phrases to the known object types.